# import pdb
from . import wigner_transform, bin_cov, parse
from . import nmt_tools
import healpy as hp
import numpy as np
import sacc
import pyccl as ccl
import sys
import os
import pymaster as nmt
import warnings

cwd = os.getcwd()
sys.path.append(os.path.dirname(cwd)+"/tjpcov")


d2r = np.pi/180


class CovarianceCalculator():
    def __init__(self,
                 tjpcov_cfg=None):
        """
        Covariance Calculator object for TJPCov.

        .. note::
            - the cosmo_fn parameter is always necessary
            - theta input in arcmin
            - Is reading all config from a single yaml a good option?
            - sacc passing values after scale cuts and no angbin_edges
            - angbin_edges necessary for bin averages
            - Check firecrown's way to handle survey features

        Parameters
        ----------
        tjpcov_cfg (str):
            filename and path to the TJPCov configuration yaml
            Check minimal example at: tests/data/conf_tjpcov_minimal.yaml

            This file MUST have
                - a sacc path
                - a xi_fn OR cl_fn
                - ccl.Cosmology object (in pickle format)  OR cosmology.yaml file generated by CCL
                - ...
                it contains info from the deprecated files:

                cosmo_fn(pyccl.object or str):
                    Receives the cosmo object or a the yaml filename
                    WARNING CCL Cosmo write_yaml seems to not pass
                            the transfer_function
                sacc_fn_xi/cl (None, str):
                    path to sacc file yaml
                    sacc object containing the tracers and binning to be used in covariance calculation
                window (None, dict, float):
                    If None, used window function specified in the sacc file
                    if dict, the keys are sacc tracer names and values are either HealSparse inverse variance
                    maps
                    if float it is assumed to be f_sky value

            TODO: cov_type = gauss
                  params
                  bias
                  fsky/mask
        """
        if isinstance(tjpcov_cfg, dict):
            config = tjpcov_cfg
        else:
            config, _ = parse(tjpcov_cfg)

        use_mpi = config['tjpcov'].get('use_mpi', False)
        if use_mpi:
            try:
                import mpi4py.MPI
            except ImportError:
                raise ValueError("MPI option requires mpi4py to be installed")

            self.comm = mpi4py.MPI.COMM_WORLD
            self.rank = self.comm.Get_rank()
            self.size = self.comm.Get_size()
        else:
            self.comm = None
            self.rank = None
            self.size = None

        self.do_xi = config['tjpcov'].get('do_xi')

        if not isinstance(self.do_xi, bool):
            raise Exception("Err: check if you set do_xi: False (Harmonic Space) "
                            + "or do_xi: True in 'tjpcov' field of your yaml")

        print("Starting TJPCov covariance calculator for", end=' ')
        print("Configuration space" if self.do_xi else "Harmonic Space")

        if self.do_xi:
            xi_fn = config['tjpcov'].get('xi_file')
        else:
            cl_fn = config['tjpcov'].get('cl_file')

        cosmo_fn = config['tjpcov'].get('cosmo')
        # sacc_fn  = config['tjpcov'].get('sacc_file')

        # biases
        # reading values w/o passing the number of tomographic bins
        # import pdb; pdb.set_trace()
        self.bias_lens = {k.replace('bias_',''):v for k,v in config['tjpcov'].items()
                            if 'bias_' in k}
        self.IA = config['tjpcov'].get('IA')
        self.Ngal = {k.replace('Ngal_',''):v*3600/d2r**2 for k, v in config['tjpcov'].items()
                            if 'Ngal' in k}
        # self.Ngal_src = {k.replace('Ngal_',''):v*3600/d2r**2 for k, v in config['tjpcov'].items()
        #                     if 'Ngal_src' in k}
        self.sigma_e = {k.replace('sigma_e_',''):v for k, v in config['tjpcov'].items()
                            if 'sigma_e' in k}


        # Treating fsky = 1 if no input is given
        self.fsky = config['tjpcov'].get('fsky')
        if self.fsky is None:
            print("No input for fsky. Assuming ", end='')
            self.fsky=1

        print(f"fsky={self.fsky}")


        if cosmo_fn is None or cosmo_fn == 'set':
            self.cosmo = self.set_ccl_cosmo(config)
        elif isinstance(cosmo_fn, ccl.core.Cosmology):
            self.cosmo = cosmo_fn
        elif cosmo_fn.split('.')[-1] in ['yaml', 'yml']:
            self.cosmo = ccl.Cosmology.read_yaml(cosmo_fn)
            # TODO: remove this hot fix of ccl
            # self.cosmo.config.transfer_function_method = 1
        elif cosmo_fn.split('.')[-1]  == 'pkl':
            import pickle
            with open(cosmo_fn, 'rb') as ccl_cosmo_file:
                self.cosmo = pickle.load(ccl_cosmo_file)
        else:
            raise Exception(
                "Err: File for cosmo field in input not recognized")

        # TO DO: remove this hotfix
        self.xi_data, self.cl_data = None, None

        if self.do_xi:
            self.xi_data = sacc.Sacc.load_fits(
                config['tjpcov'].get('sacc_file'))

        # TO DO: remove this dependence here
        #elif not do_xi:
        cl_data = config['tjpcov'].get('cl_file')
        if isinstance(cl_data, sacc.Sacc):
            self.cl_data = cl_data
        else:
            self.cl_data = sacc.Sacc.load_fits(cl_data)

        self.binning_info = config['tjpcov'].get('binning_info', None)
        if self.binning_info is None:
            # TO DO: remove this dependence here
            trcomb = self.cl_data.get_tracer_combinations()[0]
            ell_list = self.get_ell_theta(self.cl_data,  # fix this
                                          'galaxy_density_cl',
                                          trcomb,
                                          'linear', do_xi=False)
            # fao Set this inside get_ell_theta ?
            # ell, ell_bins, ell_edges = None, None, None
            theta, theta_bins, theta_edges = None, None, None

            # fix this for getting from the sacc file:
            th_list = self.set_ell_theta(2.5, 250., 20, do_xi=True)

            self.theta,  self.theta_bins, self.theta_edges,  = th_list


            # ell is the value for WT
            self.ell, self.ell_bins, self.ell_edges = ell_list
        elif self.binning_info == 'ignore':
            # Dirty trick to avoid inputting binning and not computing anything
            # of the above
            self.binning_info = None
        elif not isinstance(self.binning_info, nmt.NmtBin):
            raise ValueError('If passed, binning_info has to be a NmtBin ' +
                             'instance')

        self.mask_fn = config['tjpcov'].get('mask_file')  # windown handler TBD
        self.mask_names = config['tjpcov'].get('mask_names')

        # nside is needed if mask_fn is a hdf5 file
        self.nside = config['tjpcov'].get('nside', None)


        # Calling WT in method, only if do_xi
        self.WT = None

        # Output directory where to save all the time consuming calculations
        self.outdir = config['tjpcov'].get('outdir', None)
        if self.outdir and not os.path.isdir(self.outdir):
            os.makedirs(self.outdir)

        self.nmt_conf = config.get('NaMaster', {})
        for k in ['f', 'w', 'cw']:
            if k not in self.nmt_conf:
                self.nmt_conf[k] = {}

        # Read cache from input file. It will update the cache passed as an
        # argument of the different methods
        self.cache = config.get('cache', {})

        # SSC config
        self.ssc_conf = config.get('SSC', {})

        # Covariances requested. Input must be a string or a list of strings
        self.cov_tbc = config['tjpcov'].get('cov_type', [])
        if isinstance(self.cov_tbc, str):
            self.cov_tbc = [self.cov_tbc]

        return

    def split_tasks_by_rank(self, tasks):
        """
        Iterate through a list of items, yielding ones this process is responsible for/
        Tasks are allocated in a round-robin way.
        Parameters
        ----------
        tasks: iterable
            Tasks to split up
        """
        # Copied from https://github.com/LSSTDESC/ceci/blob/7043ae5776d9b2c210a26dde6f84bcc2366c56e7/ceci/stage.py#L586

        for i, task in enumerate(tasks):
            if self.rank is None:
                yield task
            elif i % self.size == self.rank:
                yield task

    def print_setup(self, output=None):
        """
        Placeholder of function to return setup
        TODO: Check the current setup for TJPCovs
        """
        cosmo = self.cosmo
        ell = self.ell
        if self.do_xi:
            bins = self.theta_bins
        else:
            bins = self.ell_bins
        run_configuration = {
        'do_xi': self.do_xi,
        'bins': bins
        }
        # TODO: save as yaml output
        if isinstance(output, str):
            with open(output, 'w') as ff:
                ff.write('....txt')


    def set_ccl_cosmo(self, config):
        """
        set the ccl cosmo from paramters in config file

        """
        print("Setting up cosmology...")

        cosmo_param_names = ['Omega_c', 'Omega_b', 'h',
                             'sigma8', 'n_s', 'transfer_function']
        cosmo_params = {name: config['parameters'][name]
                        for name in cosmo_param_names}
        cosmo = ccl.Cosmology(**cosmo_params)
        return cosmo


    def set_ell_theta(self, ang_min, ang_max, n_ang,
                      ang_scale='linear', do_xi=False):
        """
        Utility for return custom theta/ell bins (outside sacc)

        Parameters:
        -----------
        ang_min (int, float):
            if do_xi, ang is assumed to be theta (arcmin)
            if do_xi == False,  ang is assumed to be ell
        Returns:
        --------
            (theta, theta_edges ) (degrees):
        """
        # FIXME:
        # Use sacc is passing this
        if not do_xi:
            ang_delta = (ang_max-ang_min)//n_ang
            ang_edges = np.arange(ang_min, ang_max+1, ang_delta)
            ang = np.arange(ang_min, ang_max + ang_delta - 2)

        if do_xi:
            th_min = ang_min/60  # in degrees
            th_max = ang_max/60
            n_th_bins = n_ang
            ang_edges = np.logspace(np.log10(th_min), np.log10(th_max),
                                    n_th_bins+1)
            th = np.logspace(np.log10(th_min*0.98), np.log10(1), n_th_bins*30)
            # binned covariance can be sensitive to the th values. Make sure
            # you check convergence for your application
            th2 = np.linspace(1, th_max*1.02, n_th_bins*30)

            ang = np.unique(np.sort(np.append(th, th2)))
            ang_bins = 0.5 * (ang_edges[1:] + ang_edges[:-1])

            return ang, ang_bins, ang_edges  # TODO FIXIT

        return ang, ang_edges


    def get_ell_theta(self, two_point_data, data_type, tracer_comb, ang_scale,
                      do_xi=False):
        """
        Get ell or theta for bins given the sacc object
        For now, presuming only log and linear bins

        Parameters:
        -----------

        Returns:
        --------
        """
        ang_name = "ell" if not do_xi else 'theta'

        # assuming same ell for all bins:
        data_types = two_point_data.get_data_types()
        ang_bins = two_point_data.get_tag(ang_name, data_type=data_types[0],
                                          tracers=tracer_comb)

        ang_bins = np.array(ang_bins)

        angb_min, angb_max = ang_bins.min(), ang_bins.max()
        if ang_name == 'theta':
            # assuming log bins
            del_ang = (ang_bins[1:]/ang_bins[:-1]).mean()
            ang_scale = 'log'
            assert 1 == 1

        elif ang_name == 'ell':
            # assuming linear bins
            del_ang = (ang_bins[1:] - ang_bins[:-1])[0]
            ang_scale = 'linear'

        ang, ang_edges = self.set_ell_theta(angb_min-del_ang/2,
                                            angb_max+del_ang/2,
                                            len(ang_bins),
                                            ang_scale=ang_scale, do_xi=do_xi)
        # Sanity check
        if ang_scale == 'linear':
            assert np.allclose((ang_edges[1:]+ang_edges[:-1])/2, ang_bins), \
                "differences in produced ell/theta"
        return ang, ang_bins, ang_edges


    def wt_setup(self, ell, theta):
        """
        Set this up once before the covariance evaluations

        Parameters:
        -----------
        ell (array): array of multipoles
        theta ( array): array of theta in degrees

        Returns:
        --------
        """
        # ell = two_point_data.metadata['ell']
        # theta_rad = two_point_data.metadata['th']*d2r
        # get_ell_theta()

        WT_factors = {}
        WT_factors['lens', 'source'] = (0, 2)
        WT_factors['source', 'lens'] = (2, 0)  # same as (0,2)
        WT_factors['source', 'source'] = {'plus': (2, 2), 'minus': (2, -2)}
        WT_factors['lens', 'lens'] = (0, 0)

        self.WT_factors = WT_factors

        ell = np.array(ell)
        if not np.alltrue(ell > 1):
            # fao check warnings in WT for ell < 2
            print("Removing ell=1 for Wigner Transformation")
            ell = ell[(ell > 1)]

        WT_kwargs = {'l': ell,
                     'theta': theta*d2r,
                     's1_s2': [(2, 2), (2, -2), (0, 2), (2, 0), (0, 0)]}

        WT = wigner_transform(**WT_kwargs)
        return WT


    def get_cov_WT_spin(self, tracer_comb=None):
        """
        Parameters:
        -----------
        tracer_comb (str, str): tracer combination in sacc format

        Returns:
        --------
        WT_factors:

        """
    #     tracers=tuple(i.split('_')[0] for i in tracer_comb)
        tracers = []
        for i in tracer_comb:
            if 'lens' in i:
                tracers += ['lens']
            if 'src' in i:
                tracers += ['source']
        return self.WT_factors[tuple(tracers)]


    def get_tracer_info(self, two_point_data={}, return_noise_coupled=False):
        """
        Creates CCL tracer objects and computes the noise for all the tracers
        Check usage: Can we call all the tracer at once?

        Parameters:
        -----------
            two_point_data (sacc obj):

            return_noise_coupled (bool): If True, also return
            tracers_Noise_coupled. Default False.

        Returns:
        --------
            ccl_tracers: dict, ccl obj
                ccl.WeakLensingTracer or ccl.NumberCountsTracer
            tracer_Noise ({dict: float}):
                shot (shape) noise for lens (sources)
            tracer_Noise_coupled ({dict: float}):
                coupled shot (shape) noise for lens (sources). Returned if
                retrun_noise_coupled is True.

        """
        ccl_tracers = {}
        tracer_Noise = {}
        tracer_Noise_coupled = {}

        for tracer in two_point_data.tracers:
            tracer_dat = two_point_data.get_tracer(tracer)
            tracer_Noise_coupled[tracer] = tracer_dat.metadata.get('n_ell_coupled', None)
            # z = tracer_dat.z

            # FIXME: Following should be read from sacc dataset.--------------
            #Ngal = 26.  # arc_min^2
            #sigma_e = .26
            #b = 1.5*np.ones(len(z))  # Galaxy bias (constant with scale and z)
            # AI = .5*np.ones(len(z))  # Galaxy bias (constant with scale and z)
            #Ngal = Ngal*3600/d2r**2
            # ---------------------------------------------------------------

            # dNdz = tracer_dat.nz
            # dNdz /= (dNdz*np.gradient(z)).sum()
            # dNdz *= self.Ngal[tracer]
            #FAO  this should be called by tomographic bin
            if (tracer_dat.quantity == 'galaxy_shear') or ('src' in tracer) \
                    or ('source' in tracer):
                z = tracer_dat.z
                dNdz = tracer_dat.nz
                if self.IA is None:
                    ia_bias = None
                else:
                    IA_bin = self.IA*np.ones(len(z)) # fao: refactor this
                    ia_bias = (z, IA_bin)
                try:
                    ccl_tracers[tracer] = ccl.WeakLensingTracer(
                        self.cosmo, dndz=(z, dNdz), ia_bias=ia_bias)
                except ccl.errors.CCLError:
                    # Hack as in TXPipe. Error probably because Nz is noisy
                    print("To avoid a CCL_ERROR_INTEG we reduce the " +
                          f"number of points in the nz by half in {tracer}")
                    ccl_tracers[tracer] = ccl.WeakLensingTracer(
                        self.cosmo, dndz=(z[::2], dNdz[::2]), ia_bias=ia_bias)
                # CCL automatically normalizes dNdz
                if tracer in self.sigma_e:
                    tracer_Noise[tracer] = self.sigma_e[tracer]**2/self.Ngal[tracer]
                else:
                    tracer_Noise[tracer] = None

            elif (tracer_dat.quantity == 'galaxy_density') or \
                ('lens' in tracer):
                z = tracer_dat.z
                dNdz = tracer_dat.nz
                # import pdb; pdb.set_trace()
                b = self.bias_lens[tracer] * np.ones(len(z))
                ccl_tracers[tracer] = ccl.NumberCountsTracer(
                    self.cosmo, has_rsd=False, dndz=(z, dNdz), bias=(z, b))
                if tracer in self.Ngal:
                    tracer_Noise[tracer] = 1./self.Ngal[tracer]
                else:
                    tracer_Noise[tracer] = None
            elif tracer_dat.quantity == 'cmb_convergence':
                ccl_tracers[tracer] = ccl.CMBLensingTracer(self.cosmo,
                                                           z_source=1100)

        if not np.all(list(tracer_Noise.values())):
            warnings.warn('Missing noise for some tracers in file. You will ' +
                          'have to pass it with the cache')

        if return_noise_coupled:
            vals = list(tracer_Noise_coupled.values())
            if not np.all(vals):
                tracer_Noise_coupled = None
            elif not np.all(vals):
                warnings.warn('Missing n_ell_coupled info for some tracers in '
                              + 'the sacc file. You will have to pass it with'
                              + 'the cache')
            return ccl_tracers, tracer_Noise, tracer_Noise_coupled

        return ccl_tracers, tracer_Noise

    def get_SSC_cov(self, tracer_comb1=None, tracer_comb2=None,
                    ccl_tracers=None,
                    integration_method='qag_quad', include_b_modes=True):
        """
        Compute a single SSC covariance matrix for a given pair of C_ell. If
        outdir is set, it will save the covariance to a file called
        `ssc_tr1_tr2_tr3_tr4.npz`. This file will be read and its output
        returned if found.

        Blocks of the B-modes are assumed 0 so far.

        Parameters:
        -----------
            tracer_comb 1 (list): List of the pair of tracer names of C_ell^1
            tracer_comb 2 (list): List of the pair of tracer names of C_ell^2
            ccl_tracers (dict): Dictionary with necessary ccl_tracers with keys
            the tracer names
            integration_method (string): integration method to be used
                for the Limber integrals. Possibilities: 'qag_quad' (GSL's `qag`
                method backed up by `quad` when it fails) and 'spline' (the
                integrand is splined and then integrated analytically).
            include_b_modes (bool): If True, return the full SSC with zeros in
            for B-modes (if any). If False, return the non-zero block.

        Returns:
        --------
            cov (dict):  Super sample covariance matrix for a pair of C_ell.
            keys are 'final' and 'final_b'. The covariance stored is the same
            in both cases.

        """
        fname = 'ssc_{}_{}_{}_{}.npz'.format(*tracer_comb1, *tracer_comb2)
        fname = os.path.join(self.outdir, fname)
        if os.path.isfile(fname):
            cf = np.load(fname)
            return cf['cov' if include_b_modes else 'cov_nob']

        tr = {}
        tr[1], tr[2] = tracer_comb1
        tr[3], tr[4] = tracer_comb2

        cosmo = self.cosmo
        mass_def = ccl.halos.MassDef200m()
        hmf = ccl.halos.MassFuncTinker08(cosmo,
                                         mass_def=mass_def)
        hbf = ccl.halos.HaloBiasTinker10(cosmo,
                                         mass_def=mass_def)
        nfw = ccl.halos.HaloProfileNFW(ccl.halos.ConcentrationDuffy08(mass_def),
                                       fourier_analytic=True)
        hmc = ccl.halos.HMCalculator(cosmo, hmf, hbf, mass_def)

        # Get range of redshifts. z_min = 0 for compatibility with the limber
        # integrals
        z_max = []
        for i in range(4):
            tr_sacc = self.cl_data.tracers[tr[i + 1]]
            z, nz = tr_sacc.z, tr_sacc.nz
            # z_min.append(z[np.where(nz > 0)[0][0]])
            # z_max.append(z[np.where(np.cumsum(nz)/np.sum(nz) > 0.999)[0][0]])
            z_max.append(z.max())

        z_max = np.min(z_max)

        # Array of a.
        # Use the a's in the pk spline
        na = ccl.ccllib.get_pk_spline_na(cosmo.cosmo)
        a, _ = ccl.ccllib.get_pk_spline_a(cosmo.cosmo, na, 0)
        a = a[1/a < z_max + 1]

        bias1 = self.bias_lens.get(tr[1], 1)
        bias2 = self.bias_lens.get(tr[2], 1)
        bias3 = self.bias_lens.get(tr[3], 1)
        bias4 = self.bias_lens.get(tr[4], 1)

        isnc1 = isinstance(ccl_tracers[tr[1]], ccl.NumberCountsTracer)
        isnc2 = isinstance(ccl_tracers[tr[2]], ccl.NumberCountsTracer)
        isnc3 = isinstance(ccl_tracers[tr[3]], ccl.NumberCountsTracer)
        isnc4 = isinstance(ccl_tracers[tr[4]], ccl.NumberCountsTracer)

        tk3D = ccl.halos.halomod_Tk3D_SSC_linear_bias(cosmo=cosmo, hmc=hmc,
                                                      prof=nfw,
                                                      bias1=bias1,
                                                      bias2=bias2,
                                                      bias3=bias3,
                                                      bias4=bias4,
                                                      is_number_counts1=isnc1,
                                                      is_number_counts2=isnc2,
                                                      is_number_counts3=isnc3,
                                                      is_number_counts4=isnc4,
                                                      )

        mn = nmt_tools.get_mask_names_dict(self.mask_names, tr)
        masks = nmt_tools.get_masks_dict(self.mask_fn, mn, tr, {}, self.nside)
        # TODO: Optimize this, avoid computing the mask_wl for all blocks.
        # Note that this is correct for same footprint cross-correlations. In
        # case of multisurvey analyses this approximation might break.
        m12 = masks[1] * masks[2]
        m34 = masks[3] * masks[4]
        area = hp.nside2pixarea(hp.npix2nside(m12.size))

        alm = hp.map2alm(m12)
        blm = hp.map2alm(m34)

        mask_wl = hp.alm2cl(alm, blm)
        mask_wl *= (2 * np.arange(mask_wl.size) + 1)
        mask_wl /= np.sum(m12) * np.sum(m34) * area**2

        sigma2_B = ccl.sigma2_B_from_mask(cosmo, a=a, mask_wl=mask_wl)

        ell = nmt_tools.get_ell_eff(self.cl_data)
        cov_ssc = ccl.covariances.angular_cl_cov_SSC(cosmo,
                                                     cltracer1=ccl_tracers[tr[1]],
                                                     cltracer2=ccl_tracers[tr[2]],
                                                     cltracer3=ccl_tracers[tr[3]],
                                                     cltracer4=ccl_tracers[tr[4]],
                                                     ell=ell,
                                                     tkka=tk3D,
                                                     sigma2_B=(a, sigma2_B),
                                                     integration_method=integration_method)

        nbpw = ell.size
        ncell1 = nmt_tools.get_tracer_comb_ncell(self.cl_data, tracer_comb1)
        ncell2 = nmt_tools.get_tracer_comb_ncell(self.cl_data, tracer_comb2)
        cov_full = np.zeros((nbpw, ncell1, nbpw, ncell2))
        cov_full[:, 0, :, 0] = cov_ssc
        cov_full = cov_full.reshape((nbpw * ncell1, nbpw * ncell2))

        np.savez_compressed(fname, cov=cov_full, cov_nob=cov_ssc)

        if not include_b_modes:
            return cov_ssc

        return cov_full

    def nmt_gaussian_cov(self, tracer_comb1=None, tracer_comb2=None,
                        ccl_tracers=None, tracer_Noise=None,
                        tracer_Noise_coupled=None, coupled=False, cache=None):
        """
        Compute a single covariance matrix for a given pair of C_ell. If outdir
        is set, it will save the covariance to a file called
        `cov_tr1_tr2_tr3_tr4.npz`. This file will be read and its output
        returned if found.

        Parameters:
        -----------
            tracer_comb 1 (list): List of the pair of tracer names of C_ell^1
            tracer_comb 2 (list): List of the pair of tracer names of C_ell^2
            ccl_tracers (dict): Dictionary with necessary ccl_tracers with keys
            the tracer names
            tracer_Noise (dict): Dictionary with necessary (uncoupled) noise
            with keys the tracer names. The values must be a float or int, not
            an array
            tracer_Noise_coupled (dict): As tracer_Noise but with coupled
            noise.
            coupled (bool): True to return the coupled Gaussian covariance
            (default False)
            cache (dict): Dictionary with the necessary workspaces and
            covariance workspaces. It accept masks (keys: 'm1', 'm2', 'm3',
            'm4'), fields (keys: 'f1', 'f2', 'f3', 'f4'), workspaces (keys:
            'w13', 'w23', 'w14', 'w24', 'w12', 'w34'), the covariance
            workspace (key: 'cw') and a NmtBin (key: 'bins').

        Returns:
        --------
            cov (dict):  Gaussian covariance matrix for a pair of C_ell. keys
            are 'final' and 'final_b'. The covariance stored is the same in
            both cases.

        """
        fname = 'cov_{}_{}_{}_{}.npz'.format(*tracer_comb1, *tracer_comb2)
        fname = os.path.join(self.outdir, fname)
        if os.path.isfile(fname):
            cf = np.load(fname)
            return {'final': cf['final'], 'final_b': cf['final_b']}

        if (tracer_Noise is not None) and (tracer_Noise_coupled is not None):
            raise ValueError('Only one tracer_Noise or tracer_Noise_coupled ' +
                             'can be given')
        if coupled:
            raise ValueError('Computing coupled covariance matrix not ' +
                             'implemented yet')

        if cache is None:
            cache = {}
        cache.update(self.cache)

        if 'bins' in cache:
            bins = cache['bins']
            if (self.binning_info is not None) and \
               (bins is not self.binning_info):
                raise ValueError('Binning passed through cache is not the ' +
                                 'same as the one passed during ' +
                                 'initialization.')
        else:
            bins = self.binning_info

        # Get nbpw and ell arrays. Doing all this stuff because the window
        # function in the sacc file might be wrong.
        nbpw = nmt_tools.get_nbpw(self.cl_data)
        nell = nmt_tools.get_nell(self.cl_data, bins, self.nside, cache)

        ell = np.arange(nell)

        if 'cosmo' in cache:
            cosmo = cache['cosmo']
        else:
            cosmo = self.cosmo

        tr = {}
        tr[1], tr[2] = tracer_comb1
        tr[3], tr[4] = tracer_comb2

        ncell = {}
        ncell[12] = nmt_tools.get_tracer_comb_ncell(self.cl_data, tracer_comb1)
        ncell[34] = nmt_tools.get_tracer_comb_ncell(self.cl_data, tracer_comb2)
        ncell[13] = nmt_tools.get_tracer_comb_ncell(self.cl_data, (tr[1], tr[3]))
        ncell[24] = nmt_tools.get_tracer_comb_ncell(self.cl_data, (tr[2], tr[4]))
        ncell[14] = nmt_tools.get_tracer_comb_ncell(self.cl_data, (tr[1], tr[4]))
        ncell[23] = nmt_tools.get_tracer_comb_ncell(self.cl_data, (tr[2], tr[3]))

        s = {}
        s[1], s[2] = nmt_tools.get_tracer_comb_spin(self.cl_data, tracer_comb1)
        s[3], s[4] = nmt_tools.get_tracer_comb_spin(self.cl_data, tracer_comb2)


        # Fiducial cl
        cl = {}
        # Noise (coupled or not)
        SN = {'coupled': tracer_Noise is None}

        if SN['coupled'] is False:
            warnings.warn("Computing the coupled noise from the uncoupled " +
                          "noise. This assumes the noise is white")

        for i in [13, 24, 14, 23]:
            # Fiducial cl
            i1, i2 = [int(j) for j in str(i)]
            key = f'cl{i}'
            if key in cache:
                cl[i] = cache[key]
            else:
                cl[i] = np.zeros((ncell[i], ell.size))
                cl[i][0] = ccl.angular_cl(cosmo, ccl_tracers[tr[i1]],
                                          ccl_tracers[tr[i2]], ell)

            # Noise
            auto = tr[i1] == tr[i2]
            key = f'SN{i}'
            if key in cache:
                SN[i] = cache[key]
            else:
                SN[i] = np.zeros((ncell[i], ell.size))
                SN[i][0] = SN[i][-1] = np.ones_like(ell)
                if SN['coupled']:
                    SN[i] *= tracer_Noise_coupled[tr[i1]] if auto else 0
                else:
                    SN[i] *= tracer_Noise[tr[i1]] if auto else 0
                if s[i1] == 2:
                    SN[i][0, :2] = SN[i][-1, :2] = 0


        if np.any(cl[13]) or np.any(cl[24]) or np.any(cl[14]) or \
                np.any(cl[23]):


            # TODO: Modify depending on how TXPipe caches things
            # Mask, mask_names, field and workspaces dictionaries
            mn = nmt_tools.get_mask_names_dict(self.mask_names, tr)
            m = nmt_tools.get_masks_dict(self.mask_fn, mn, tr, cache,
                                         self.nside)
            f = nmt_tools.get_fields_dict(m, s, mn, tr, self.nmt_conf['f'],
                                          cache)
            w = nmt_tools.get_workspaces_dict(f, m, mn, bins, self.outdir,
                                              self.nmt_conf['w'], cache)

            # TODO; Allow input options as output folder, if recompute, etc.
            if 'cw' in cache:
                cw = cache['cw']
            else:
                cw = nmt_tools.get_covariance_workspace(f[1], f[2], f[3], f[4],
                                                        mn[1], mn[2], mn[3],
                                                        mn[4], self.outdir,
                                                        lmax=int(ell[-1]),
                                                        **self.nmt_conf['cw'])

            cl_cov = {}
            cl_cov[13] = nmt_tools.get_cl_for_cov(cl[13], SN[13], m[1], m[3],
                                                  w[13], nl_is_cp=SN['coupled'])
            cl_cov[23] = nmt_tools.get_cl_for_cov(cl[23], SN[23], m[2], m[3],
                                                  w[23], nl_is_cp=SN['coupled'])
            cl_cov[14] = nmt_tools.get_cl_for_cov(cl[14], SN[14], m[1], m[4],
                                                  w[14], nl_is_cp=SN['coupled'])
            cl_cov[24] = nmt_tools.get_cl_for_cov(cl[24], SN[24], m[2], m[4],
                                                  w[24], nl_is_cp=SN['coupled'])

            cov = nmt.gaussian_covariance(cw, s[1], s[2], s[3], s[4],
                                          cl_cov[13], cl_cov[14], cl_cov[23],
                                          cl_cov[24], w[12], w[34], coupled)
        else:
            size1 = ncell[12] * nbpw
            size2 = ncell[34] * nbpw
            cov = np.zeros((size1, size2))

        np.savez_compressed(fname, cov=cov, final=cov, final_b=cov)

        return {'final': cov, 'final_b': cov}

    def compute_all_blocks_nmt(self, tracer_noise, tracer_noise_coupled,
                               **kwargs):
        """
        Compute all the independent covariance blocks.
        Parameters:
        -----------
        tracer_noise (dict): Dictionary with necessary (uncoupled) noise
        with keys the tracer names. The values must be a float or int, not
        an array
        tracer_noise_coupled (dict): As tracer_Noise but with coupled
        noise.
        **kwargs: The arguments to pass to your chosen covariance estimation
        method.

        Returns:
        --------
        blocks (list):
            List of all the independent covariance blocks.
        """

        if (tracer_noise is not None) and (tracer_noise_coupled is not None):
            raise ValueError('Only one of tracer_nose or ' +
                             'tracer_noise_coupled can be given')

        two_point_data = self.cl_data
        cl_tracers = two_point_data.get_tracer_combinations()

        ccl_tracers, tracer_Noise, tracer_Noise_coupled = \
                self.get_tracer_info(two_point_data, return_noise_coupled=True)

        if (tracer_noise_coupled is not None) or \
                (tracer_Noise_coupled is not None):
            tracer_Noise = None
            if tracer_Noise_coupled is None:
                tracer_Noise_coupled = {}
        else:
            tracer_Noise_coupled = None


        # Circunvent the impossibility of inputting noise by hand
        for tracer in ccl_tracers:
            if tracer_noise and tracer in tracer_noise:
                tracer_Noise[tracer] = tracer_noise[tracer]
            elif tracer_noise_coupled and tracer in tracer_noise_coupled:
                tracer_Noise_coupled[tracer] = tracer_noise_coupled[tracer]

        # Make a list of all pair of tracer combinations needed to compute the
        # independent workspaces
        trs_wsp = nmt_tools.get_list_of_tracers_for_wsp(two_point_data,
                                                        self.mask_names)
        # Now the tracers for covariance workspaces (without trs_wsp)
        trs_cwsp = nmt_tools.get_list_of_tracers_for_cov_wsp(two_point_data,
                                                             self.mask_names,
                                                             remove_trs_wsp=True)

        # Make a list of all remaining combinations
        tracers_cov = nmt_tools.get_list_of_tracers_for_cov(two_point_data,
                                                            remove_trs_wsp_cwsp=True,
                                                            mask_names=self.mask_names)

        # Save blocks and the corresponding tracers, as comm.gather does not
        # return the blocks in the original order.
        blocks = []
        tracers_blocks = []
        print('Computing independent covariance blocks')
        print('Computing the blocks for independent workspaces')
        for tracer_comb1, tracer_comb2 in self.split_tasks_by_rank(trs_wsp):
            print(tracer_comb1, tracer_comb2)
            cov = self.nmt_gaussian_cov(tracer_comb1=tracer_comb1,
                                        tracer_comb2=tracer_comb2,
                                        ccl_tracers=ccl_tracers,
                                        tracer_Noise=tracer_Noise,
                                        tracer_Noise_coupled=tracer_Noise_coupled,
                                        **kwargs)['final']
            blocks.append(cov)
            tracers_blocks.append((tracer_comb1, tracer_comb2))

        if self.comm:
            self.comm.Barrier()

        print('Computing the blocks for independent covariance workspaces')
        for tracer_comb1, tracer_comb2 in self.split_tasks_by_rank(trs_cwsp):
            print(tracer_comb1, tracer_comb2)
            cov = self.nmt_gaussian_cov(tracer_comb1=tracer_comb1,
                                        tracer_comb2=tracer_comb2,
                                        ccl_tracers=ccl_tracers,
                                        tracer_Noise=tracer_Noise,
                                        tracer_Noise_coupled=tracer_Noise_coupled,
                                        **kwargs)['final']
            blocks.append(cov)
            tracers_blocks.append((tracer_comb1, tracer_comb2))

        if self.comm:
            self.comm.Barrier()

        print('Computing the remaining blocks')
        # Now loop over the remaining tracers
        for tracer_comb1, tracer_comb2 in self.split_tasks_by_rank(tracers_cov):
            print(tracer_comb1, tracer_comb2)
            cov = self.nmt_gaussian_cov(tracer_comb1=tracer_comb1,
                                        tracer_comb2=tracer_comb2,
                                        ccl_tracers=ccl_tracers,
                                        tracer_Noise=tracer_Noise,
                                        tracer_Noise_coupled=tracer_Noise_coupled,
                                        **kwargs)['final']
            blocks.append(cov)
            tracers_blocks.append((tracer_comb1, tracer_comb2))


        return blocks, tracers_blocks

    def compute_all_blocks_SSC(self,
                               integration_method='qag_quad',
                               include_b_modes=True):
        """
        Compute all the independent super sample covariance blocks.

        Parameters:
        -----------
        integration_method (string) : integration method to be used for the
        Limber integrals. Possibilities: 'qag_quad' (GSL's `qag` method backed
        up by `quad` when it fails) and 'spline' (the integrand is splined and
        then integrated analytically).
        include_b_modes (bool): If True, return the full SSC with zeros in for
        B-modes (if any). If False, return the non-zero block.

        Returns:
        --------
        blocks (list): List of all the independent super sample covariance
        blocks.
        """

        two_point_data = self.cl_data
        ccl_tracers, _ = self.get_tracer_info(two_point_data)

        # Make a list of all independent tracer combinations
        tracers_cov = nmt_tools.get_list_of_tracers_for_cov(two_point_data)

        # Save blocks and the corresponding tracers, as comm.gather does not
        # return the blocks in the original order.
        blocks = []
        tracers_blocks = []
        print('Computing independent covariance blocks')
        for tracer_comb1, tracer_comb2 in self.split_tasks_by_rank(tracers_cov):
            print(tracer_comb1, tracer_comb2)
            cov = self.get_SSC_cov(tracer_comb1=tracer_comb1,
                                   tracer_comb2=tracer_comb2,
                                   ccl_tracers=ccl_tracers,
                                   integration_method=integration_method,
                                   include_b_modes=include_b_modes)
            blocks.append(cov)
            tracers_blocks.append((tracer_comb1, tracer_comb2))


        return blocks, tracers_blocks

    def cl_gaussian_cov(self, tracer_comb1=None, tracer_comb2=None,
                        ccl_tracers=None, tracer_Noise=None,
                        two_point_data=None, do_xi=False,
                        xi_plus_minus1='plus', xi_plus_minus2='plus'):
        """
        Compute a single covariance matrix for a given pair of C_ell or xi

        Returns:
        --------
            final:  unbinned covariance for C_ell
            final_b : binned covariance
        """
        # fsky should be read from the sacc
        # tracers 1,2,3,4=tracer_comb1[0],tracer_comb1[1],tracer_comb2[0],tracer_comb2[1]
        # ell=two_point_data.metadata['ell']
        # fao to discuss: indices
        cosmo = self.cosmo
        #do_xi=self.do_xi

        if not do_xi:
            ell = self.ell
        else:
            # FIXME:  check the max_ell here in the case of only xi
            ell = self.ell

        cl = {}
        cl[13] = ccl.angular_cl(
            cosmo, ccl_tracers[tracer_comb1[0]], ccl_tracers[tracer_comb2[0]], ell)
        cl[24] = ccl.angular_cl(
            cosmo, ccl_tracers[tracer_comb1[1]], ccl_tracers[tracer_comb2[1]], ell)
        cl[14] = ccl.angular_cl(
            cosmo, ccl_tracers[tracer_comb1[0]], ccl_tracers[tracer_comb2[1]], ell)
        cl[23] = ccl.angular_cl(
            cosmo, ccl_tracers[tracer_comb1[1]], ccl_tracers[tracer_comb2[0]], ell)

        SN = {}
        SN[13] = tracer_Noise[tracer_comb1[0]
                              ] if tracer_comb1[0] == tracer_comb2[0] else 0
        SN[24] = tracer_Noise[tracer_comb1[1]
                              ] if tracer_comb1[1] == tracer_comb2[1] else 0
        SN[14] = tracer_Noise[tracer_comb1[0]
                              ] if tracer_comb1[0] == tracer_comb2[1] else 0
        SN[23] = tracer_Noise[tracer_comb1[1]
                              ] if tracer_comb1[1] == tracer_comb2[0] else 0

        if do_xi:
            norm = np.pi*4* self.fsky #two_point_data.metadata['fsky']
        else:  # do c_ell
            norm = (2*ell+1)*np.gradient(ell)* self.fsky #two_point_data.metadata['fsky']

        coupling_mat = {}
        coupling_mat[1324] = np.eye(len(ell))  # placeholder
        coupling_mat[1423] = np.eye(len(ell))  # placeholder

        cov = {}
        cov[1324] = np.outer(cl[13]+SN[13], cl[24]+SN[24])*coupling_mat[1324]
        cov[1423] = np.outer(cl[14]+SN[14], cl[23]+SN[23])*coupling_mat[1423]

        cov['final'] = cov[1423]+cov[1324]

        if do_xi:
            if self.WT is None:  # class modifier of WT initialization
                print("Preparing WT...")
                self.WT = self.wt_setup(self.ell, self.theta)
                print("Done!")

            # Fixme: SET A CUSTOM ELL FOR do_xi case, in order to use
            # a single sacc input filefile
            ell = self.ell
            s1_s2_1 = self.get_cov_WT_spin(tracer_comb=tracer_comb1)
            s1_s2_2 = self.get_cov_WT_spin(tracer_comb=tracer_comb2)
            if isinstance(s1_s2_1, dict):
                s1_s2_1 = s1_s2_1[xi_plus_minus1]
            if isinstance(s1_s2_2, dict):
                s1_s2_2 = s1_s2_2[xi_plus_minus2]
            th, cov['final'] = self.WT.projected_covariance2(l_cl=ell, s1_s2=s1_s2_1,
                                                             s1_s2_cross=s1_s2_2,
                                                             cl_cov=cov['final'])

        cov['final'] /= norm

        if do_xi:
            thb, cov['final_b'] = bin_cov(
                r=th/d2r, r_bins=self.theta_edges, cov=cov['final'])
            # r=th/d2r, r_bins=two_point_data.metadata['th_bins'], cov=cov['final'])
        else:
            # if two_point_data.metadata['ell_bins'] is not None:
            if self.ell_edges is not None:
                lb, cov['final_b'] = bin_cov(
                    r=self.ell, r_bins=self.ell_edges, cov=cov['final'])
                # r=ell, r_bins=two_point_data.metadata['ell_bins'], cov=cov['final'])

    #     cov[1324]=None #if want to save memory
    #     cov[1423]=None #if want to save memory
        return cov

    def get_all_cov(self, do_xi=False, use_nmt=False, **kwargs):
        """
        Compute all the covariances and then combine them into one single giant matrix
        Parameters:
        -----------
        two_point_data (sacc obj): sacc object containg two_point data
        **kwargs: The arguments to pass to your chosen covariance estimation
        method.

        Returns:
        --------
        cov_full (Npt x Npt numpy array):
            Covariance matrix for all combinations.
            Npt = (number of bins ) * (number of combinations)

        """
        # FIXME: Only input needed should be two_point_data,
        # which is the sacc data file. Other parameters should be
        # included within sacc and read from there.
        # Check: now it is evaluating all the blocks based on sacc file. It seems
        # better to get all the blocks based on yaml file (the user not needing to
        # modify the sacc/fits file.)
        
        if use_nmt:
            raise ValueError('This function does not work with the NaMaster' +
                             'wrapper at the moment. Use get_all_cov_nmt.')

        two_point_data = self.xi_data if do_xi else self.cl_data

        ccl_tracers, tracer_Noise = self.get_tracer_info(
            two_point_data=two_point_data)

        # we will loop over all these
        tracer_combs = two_point_data.get_tracer_combinations()
        N2pt = len(tracer_combs)

        N_data = len(two_point_data.indices())
        print(f"Producing covariance with {N_data}x{N_data} points", end=" ")
        print(f"({N2pt} combinations of tracers)")

        # if two_point_data.metadata['ell_bins'] is not None:
        #     Nell_bins = len(two_point_data.metadata['ell_bins'])-1
        # else:
        #     Nell_bins = len(two_point_data.metadata['ell'])

        # if do_xi:
        #     Nell_bins = len(two_point_data.metadata['th_bins'])-1

        # cov_full = np.zeros((Nell_bins*N2pt, Nell_bins*N2pt))

        cov_full = np.zeros((N_data, N_data))

        # Fix this loop for uneven scale cuts (different N_ell)
        for i in np.arange(N2pt):
            print("{}/{}".format(i+1, N2pt))
            tracer_comb1 = tracer_combs[i]
            # solution for non-equal number of ell in bins
            Nell_bins_i = len(two_point_data.indices(tracers=tracer_comb1))
            indx_i = i*Nell_bins_i
            for j in np.arange(i, N2pt):
                tracer_comb2 = tracer_combs[j]
                Nell_bins_j = len(two_point_data.indices(tracers=tracer_comb2))
                indx_j = j*Nell_bins_j
                if use_nmt:
                    cov_ij = self.nmt_gaussian_cov(tracer_comb1=tracer_comb1,
                                                   tracer_comb2=tracer_comb2,
                                                   ccl_tracers=ccl_tracers,
                                                   tracer_Noise=tracer_Noise,
                                                   **kwargs)
                else:
                    cov_ij = self.cl_gaussian_cov(tracer_comb1=tracer_comb1,
                                                  tracer_comb2=tracer_comb2,
                                                  ccl_tracers=ccl_tracers,
                                                  tracer_Noise=tracer_Noise,
                                                  do_xi=do_xi,
                                                  two_point_data=two_point_data)

                # if do_xi or two_point_data.metadata['ell_bins'] is not None:
                # check
                if do_xi or self.ell_bins is not None:
                    cov_ij = cov_ij['final_b']
                else:
                    cov_ij = cov_ij['final']

                cov_full[indx_i:indx_i+Nell_bins_i,
                         indx_j:indx_j+Nell_bins_j] = cov_ij
                cov_full[indx_j:indx_j+Nell_bins_i,
                         indx_i:indx_i+Nell_bins_j] = cov_ij.T
        return cov_full

    def _build_matrix_from_blocks(self, blocks, tracers_cov):
        """
        Build full matrix from blocks.

        Parameters:
        -----------
        blocks (list): List of blocks
        tracers_cov (list): List of tracer combinations corresponding to each
        block in blocks. They should have the same order

        Returns:
        --------
        cov_full (Npt x Npt numpy array):
            Covariance matrix for all combinations.
            Npt = (number of bins ) * (number of combinations)
        """
        blocks = iter(blocks)

        two_point_data = self.cl_data
        # Covariance construction based on
        # https://github.com/xC-ell/xCell/blob/069c42389f56dfff3a209eef4d05175707c98744/xcell/cls/to_sacc.py#L86-L123
        s = nmt_tools.get_sacc_with_concise_dtypes(two_point_data)
        nbpw = nmt_tools.get_nbpw(s)
        #
        ndim = s.mean.size
        cl_tracers = s.get_tracer_combinations()

        cov_full = -1 * np.ones((ndim, ndim))

        print('Building the covariance: placing blocks in their place')
        for tracer_comb1, tracer_comb2 in tracers_cov:
            print(tracer_comb1, tracer_comb2)
            # Although these two variables do not vary as ncell2 and dtypes2,
            # it is cleaner to tho the loop this way
            ncell1 = nmt_tools.get_tracer_comb_ncell(s, tracer_comb1)
            dtypes1 = nmt_tools.get_datatypes_from_ncell(ncell1)

            ncell2 = nmt_tools.get_tracer_comb_ncell(s, tracer_comb2)
            dtypes2 = nmt_tools.get_datatypes_from_ncell(ncell2)

            cov_ij = next(blocks)
            cov_ij = cov_ij.reshape((nbpw, ncell1, nbpw, ncell2))

            for i, dt1 in enumerate(dtypes1):
                ix1 = s.indices(tracers=tracer_comb1, data_type=dt1)
                if len(ix1) == 0:
                    continue
                for j, dt2 in enumerate(dtypes2):
                    ix2 = s.indices(tracers=tracer_comb2, data_type=dt2)
                    if len(ix2) == 0:
                        continue
                    covi = cov_ij[:, i, :, j]
                    cov_full[np.ix_(ix1, ix2)] = covi
                    cov_full[np.ix_(ix2, ix1)] = covi.T

        if np.any(cov_full == -1):
            raise Exception('Something went wrong. Probably related to the ' +
                            'data types')

        return cov_full

    def get_all_cov_nmt(self, tracer_noise=None, tracer_noise_coupled=None,
                        **kwargs):
        """
        Compute all the covariances and then combine them into one single giant matrix
        Parameters:
        -----------
        tracer_noise (dict): Dictionary with necessary (uncoupled) noise
        with keys the tracer names. The values must be a float or int, not
        an array
        tracer_noise_coupled (dict): As tracer_Noise but with coupled
        noise.
        **kwargs: The arguments to pass to your chosen covariance estimation
        method.

        Returns:
        --------
        cov_full (Npt x Npt numpy array):
            Covariance matrix for all combinations.
            Npt = (number of bins ) * (number of combinations)
        """

        blocks, tracers_cov = self.compute_all_blocks_nmt(tracer_noise,
                                             tracer_noise_coupled, **kwargs)

        if self.comm is not None:
            blocks = self.comm.gather(blocks, root=0)
            tracers_cov = self.comm.gather(tracers_cov, root=0)

            if self.rank == 0:
                blocks = sum(blocks, [])
                tracers_cov = sum(tracers_cov, [])
            else:
                return

        cov_full = self._build_matrix_from_blocks(blocks, tracers_cov)

        return cov_full

    def get_all_cov_SSC(self, integration_method=None):
        """
        Compute all the SSC block covariances and then combine them into one
        single giant matrix

        Parameters:
        -----------
        integration_method (None or string) : integration method to be used
            for the Limber integrals. Possibilities: 'qag_quad' (GSL's `qag`
            method backed up by `quad` when it fails) and 'spline' (the
            integrand is splined and then integrated analytically). If None, it
            will use the configuration passed at initialization or default to
            'qag_quad' if not given.

        Returns:
        --------
        cov_full (Npt x Npt numpy array):
            Covariance matrix for all combinations.
            Npt = (number of bins ) * (number of combinations)
        """
        # If the user doesn't call this function with a integration_method, use
        # the one passed at initialization. If not given, default to qug_quad
        if integration_method is None:
            integration_method = self.ssc_conf.get('integration_method',
                                                   'qag_quad')

        blocks, tracers_cov = self.compute_all_blocks_SSC(integration_method)

        if self.comm is not None:
            blocks = self.comm.gather(blocks, root=0)
            tracers_cov = self.comm.gather(tracers_cov, root=0)

            if self.rank == 0:
                blocks = sum(blocks, [])
                tracers_cov = sum(tracers_cov, [])
            else:
                return

        cov_full = self._build_matrix_from_blocks(blocks, tracers_cov)

        return cov_full

    def get_final_cov(self, gauss_kwargs=None, ssc_kwargs=None):
        """
        Returns the covariance with all the terms requested (Gaussian, ssc,
        etc.).

        Parameters:
        ----------
        gauss_kwargs (None or dict): If given, dictionary with arguments to
        pass the get_all_cov or get_all_cov_nmt functions.
        ssc_kwargs (None or dict): If given, dictionary with arguments to
        pass the get_all_cov_SSC function.
        Returns:
        -------
        cov (array): Covariance with all the terms requested added.

        """
        if not self.cov_tbc:
            print('No covariance requested')
            return

        s = self.cl_data.copy()
        cov = np.zeros((s.mean.size, s.mean.size))

        if ('Gauss' in self.cov_tbc) or ('gauss' in self.cov_tbc):
            print('Computing Gaussian covariance')
            if gauss_kwargs is None:
                gauss_kwargs = {}

            if self.do_xi:
                cov += self.get_all_cov(**gauss_kwargs)
            else:
                cov += self.get_all_cov_nmt(**gauss_kwargs)
        
        # fao 
        if ('SSC' in self.cov_tbc) or ('ssc' in self.cov_tbc):
            print('Computing Super Sample Covariance')
            kwargs = {}
            kwargs.update(self.ssc_conf)
            if ssc_kwargs is not None:
                kwargs.update(ssc_kwargs)

            print(kwargs)
            cov += self.get_all_cov_SSC(**kwargs)

        return cov


    def create_sacc_cov(self, output, gauss_kwargs=None, ssc_kwargs=None):
        """
        Write created cov to a new sacc object

        Parameters:
        ----------
        output (str): filename output
        gauss_kwargs (None or dict): If given, dictionary with arguments to
        pass the get_all_cov or get_all_cov_nmt functions.
        ssc_kwargs (None or dict): If given, dictionary with arguments to
        pass the get_all_cov_SSC function.
        Returns:
        -------
        None

        """
        cov = self.get_final_cov()

        s = self.cl_data.copy()
        s.add_covariance(cov)
        s.save_fits(output, overwrite=True)


if __name__ == "__main__":
    import tjpcov.main as cv
    import pickle
    import sys
    import os

    cwd = os.getcwd()
    sys.path.append(os.path.dirname(cwd)+"/tjpcov")
    # reference:
    with open("./tests/data/tjpcov_cl.pkl", "rb") as ff:
        cov0cl = pickle.load(ff)


    tjp0 = cv.CovarianceCalculator(tjpcov_cfg="tests/data/conf_tjpcov_minimal.yaml")

    ccl_tracers, tracer_Noise = tjp0.get_tracer_info(tjp0.cl_data)
    trcs = tjp0.cl_data.get_tracer_combinations()

    gcov_cl_0 = tjp0.cl_gaussian_cov(tracer_comb1=('lens0', 'lens0'),
                                     tracer_comb2=('lens0', 'lens0'),
                                     ccl_tracers=ccl_tracers,
                                     tracer_Noise=tracer_Noise,
                                     two_point_data=tjp0.cl_data,
                                     )


    if np.array_equal(gcov_cl_0['final_b'].diagonal()[:], cov0cl.diagonal()[:24]):
        print("Cov (diagonal):\n", gcov_cl_0['final_b'].diagonal()[:])
    else:
        print(gcov_cl_0['final_b'].diagonal()[:], cov0cl.diagonal()[:24])


